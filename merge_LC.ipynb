{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - `merge_LC`\n",
    "\n",
    "For a given system, this code fetches its different light curves and combines them into one. Before doing so, it clears each dataset from NaNs, infinite values, and bad SAP_QUALITY points, and removes >3sigma outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = '/Users/mbadenas/Documents/Master UAB/Tesis UAB/TFM2018/clean_bat_files/LC_p13up/'\n",
    "out_path_file = '/Users/mbadenas/Documents/Master UAB/Tesis UAB/TFM2018/merge_light_curves/LC_p13up_merged/'\n",
    "\n",
    "properties_sample = pd.read_csv(path_file+'all_targets_P13up.csv', sep=',', \n",
    "                                comment='#', na_values = '\\\\N')\n",
    "\n",
    "targets = pd.read_csv(path_file+'kepler_id.txt', delimiter=',',\n",
    "                         dtype=int, header=None, names=['kepid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check files:\n",
      "*Lengths don't match: 185 1018\n",
      "\tSome systems have both sc and lc data! Remove duplicates.\n",
      "*Lengths match: 185 185\n",
      "\tDuplicates have been removed.\n",
      " A total of 185 systems have short-cadence LC with SN > 7.1 for their first transit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(185, 141)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.merge(targets, properties_sample, on=['kepid'], how='inner') \n",
    "sc_data = df.drop_duplicates('kepid') #remove duplicates (some systems have both sc and lc LC)\n",
    "\n",
    "print(\"Check files:\")\n",
    "\n",
    "if (len(targets) != len(properties_sample)):\n",
    "    print(\"*Lengths don't match:\", len(targets), len(properties_sample))\n",
    "    print(\"\\tSome systems have both sc and lc data! Remove duplicates.\")\n",
    "\n",
    "if (len(sc_data) == len(targets)):\n",
    "    print(\"*Lengths match:\", len(sc_data), len(targets))\n",
    "    print('\\tDuplicates have been removed.\\n A total of {} systems have short-cadence LC with SN > 7.1 for their first transit'.format(len(sc_data)))\n",
    "    \n",
    "sc_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "bjd_ref = 2454833\n",
    "\n",
    "def extract_LC(name):\n",
    "    id_kep = \"%.0f\" % name\n",
    "    files = glob(path_file+'kplr*'+id_kep+'*.fits')\n",
    "     \n",
    "    clean_time_bkjd = np.array([])\n",
    "    clean_time_bjd = np.array([])\n",
    "    clean_flux = np.array([])\n",
    "    clean_flux_err = np.array([])\n",
    "    detrended_flux = np.array([])\n",
    "    \n",
    "    for lc in files:\n",
    "        #print lc\n",
    "        hdulist = fits.open(lc)\n",
    "        header_fits = hdulist[0]\n",
    "        lc_info = hdulist[1]\n",
    "\n",
    "        science_data = lc_info.data\n",
    "        \n",
    "        time = science_data['TIME'] # Barycenter Kepler Julian Day, BKJD\n",
    "        flux = science_data['PDCSAP_FLUX'] #flujo corregido\n",
    "        flux_er = science_data['PDCSAP_FLUX_ERR'] #error flujo corregido\n",
    "        quality = science_data['SAP_QUALITY']\n",
    "        \n",
    "        time_bjd = time + bjd_ref  # Barycenter Julian Day, BJD\n",
    "\n",
    "        hdulist.close()\n",
    "        \n",
    "        #Clean data from NaNs and infinite values\n",
    "        mask = np.isfinite(science_data['PDCSAP_FLUX']) & (science_data['SAP_QUALITY']==0)\n",
    "        flux, flux_er = flux[mask], flux_er[mask]\n",
    "        time, time_bjd = time[mask], time_bjd[mask]\n",
    "        \n",
    "        \"\"\"\n",
    "        pos_nan = np.isnan(flux)\n",
    "        flux, flux_er = flux[~pos_nan], flux_er[~pos_nan]\n",
    "        time, time_bjd = time[~pos_nan], time_bjd[~pos_nan]\n",
    "        \"\"\"\n",
    "        #Remove 3sigma points\n",
    "        errorLC = np.std(flux)\n",
    "        goodP = flux <= np.mean(flux)+3*errorLC\n",
    "\n",
    "        #Clean data of >3 sigma outliers\n",
    "        clean_time_bkjd = np.append(clean_time_bkjd, time[goodP])\n",
    "        clean_time_bjd = np.append(clean_time_bjd, time_bjd[goodP])\n",
    "        clean_flux = np.append(clean_flux, (flux[goodP]/np.mean(flux[goodP])))\n",
    "        clean_flux_err = np.append(clean_flux_err, flux_er[goodP]/np.mean(flux_er[goodP]))\n",
    "          \n",
    "    targetLC = pd.DataFrame(OrderedDict({'Time BKJD': clean_time_bkjd, \n",
    "                                         'Flux': clean_flux, \n",
    "                                         'Flux_Err': clean_flux_err}))\n",
    "    \n",
    "    cols = list(targetLC.columns.values)\n",
    "    targetLC = targetLC[['Time BKJD', 'Flux', 'Flux_Err']]\n",
    "    \n",
    "    \n",
    "    np.savetxt(path_file+'KID'+id_kep+'.txt', targetLC.values, fmt='%f', delimiter=\"\\t\", \n",
    "               header = 'Time BKJD\\tFlux\\tFlux Err')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "extract_LC() missing 1 required positional argument: 'found'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-19aca1748b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msc_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pandas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mkepid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kepid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mextract_LC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkepid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: extract_LC() missing 1 required positional argument: 'found'"
     ]
    }
   ],
   "source": [
    "for row in sc_data.itertuples(index=True, name='Pandas'):\n",
    "    kepid = getattr(row, \"kepid\")\n",
    "    extract_LC(kepid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
